{"nbformat_minor": 0, "cells": [{"source": "# Lab 2 - Spark SQL\nThis Lab will show you how to work with Spark SQL\n", "cell_type": "markdown", "metadata": {}}, {"source": "#Step 1\n\n<h3>Getting started: Create a SQL Context</h3>\n\n<b>Type:</b>\n\n\nfrom pyspark.sql import SQLContext<br>\nsqlContext = SQLContext(sc)\n", "cell_type": "markdown", "metadata": {}}, {"execution_count": 1, "cell_type": "code", "source": "#Create the SQLContext below:\nfrom pyspark.sql import SQLContext\nsqlContext = SQLContext(sc)\n\n", "outputs": [], "metadata": {"collapsed": true, "trusted": false}}, {"source": "#Step 2\n\n<h3>Dowload a JSON Recordset to work with</h3>", "cell_type": "markdown", "metadata": {}}, {"source": "Let's download the data, we can run commands on the console of the server (or docker image) that the notebook enviroment is using. To do so we simply put a \"!\" in front of the command that we want to run. For example:\n\n!pwd\n\nTo get the data we will download a file to the enviroment. Simple run these two commands, the first just ensures that the file is removed if it exists:\n\n!rm world_bank.json.gz -f <br>\n!wget https://raw.githubusercontent.com/bradenrc/sparksql_pot/master/world_bank.json.gz", "cell_type": "markdown", "metadata": {}}, {"execution_count": 2, "cell_type": "code", "source": "#enter the commands to remove and download file here\n!rm world_bank.json.gz -f\n!wget https://raw.githubusercontent.com/bradenrc/sparksql_pot/master/world_bank.json.gz", "outputs": [{"output_type": "stream", "name": "stdout", "text": "--2016-05-05 12:38:15--  https://raw.githubusercontent.com/bradenrc/sparksql_pot/master/world_bank.json.gz\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 23.235.39.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|23.235.39.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 446287 (436K) [application/octet-stream]\nSaving to: \u2018world_bank.json.gz\u2019\n\n100%[======================================>] 446,287     --.-K/s   in 0.1s    \n\n2016-05-05 12:38:15 (3.26 MB/s) - \u2018world_bank.json.gz\u2019 saved [446287/446287]\n\n"}], "metadata": {"collapsed": false, "trusted": false}}, {"source": "#Step 3\n<h3>Create a Dataframe</h3>\n\nNow you can create the Dataframe, note that if you wanted to see where you downloaded the file you can run !pwd or !ls\n\nTo create the Dataframe type:\n\nexample1_df = sqlContext.read.json(\"/resources/world_bank.json.gz\")\n", "cell_type": "markdown", "metadata": {}}, {"execution_count": 3, "cell_type": "code", "source": "#create the Dataframe here:\nexample1_df = sqlContext.read.json(\"/resources/world_bank.json.gz\")\n\nexample1_df.", "outputs": [], "metadata": {"collapsed": false, "trusted": false}}, {"source": "<h3>We can look at the schema with this command:</h3>\n\nexample1_df.printSchema()", "cell_type": "markdown", "metadata": {}}, {"execution_count": 4, "cell_type": "code", "source": "#print out the schema\nexample1_df.printSchema()", "outputs": [], "metadata": {"collapsed": false, "trusted": false}}, {"source": "<h3>Dataframes work like RDDs, you can map, reduce, groupby, etc. \n<br>Take a look at the first two rows of data using \"take\"</h3>\n\n", "cell_type": "markdown", "metadata": {}}, {"execution_count": 5, "cell_type": "code", "source": "#Use take on the dataframe to pull out 2 rows", "outputs": [], "metadata": {"collapsed": false, "trusted": false}}, {"source": "#Step 4 \n<h3>Register a table</h3>\n\nUsing\nDataframeObject.registerTempTable(\"name_of_table\")\n\nCreate a table named \"world_bank\"", "cell_type": "markdown", "metadata": {}}, {"execution_count": 9, "cell_type": "code", "source": "#Create the table to be referenced via SQL\n\n", "outputs": [], "metadata": {"collapsed": false, "trusted": false}}, {"source": "#Step 5\n<h3>Writing SQL Statements</h3>\nUsing SQL Get the first 2 records\nsqlContext.sql(\"SQL Statement\") will return a Dataframe with the records", "cell_type": "markdown", "metadata": {}}, {"execution_count": 1, "cell_type": "code", "source": "#Use SQL to select from table limit 2 and print the output", "outputs": [], "metadata": {"collapsed": false, "trusted": false}}, {"execution_count": 2, "cell_type": "code", "source": "#Extra credit, take the Dataframe you created with the two records and convert it into Pandas", "outputs": [], "metadata": {"collapsed": false, "trusted": false}}, {"execution_count": 3, "cell_type": "code", "source": "#Now Calculate a Simple count based on a group, for example \"regionname\"", "outputs": [], "metadata": {"collapsed": false, "trusted": false}}, {"execution_count": 13, "cell_type": "code", "source": "# With JSON data you can reference the nested data\n# If you look at Schema above you can see that Sector.Name is a nested column\n# Select that column and limit to reasonable output (like 2)\n\nprint sqlContext.sql(\"select id, sector.Name from world_bank limit 2\").collect()", "outputs": [{"output_type": "stream", "name": "stdout", "text": "[Row(id=u'P129828', Name=[u'Primary education', u'Secondary education', u'Public administration- Other social services', u'Tertiary education']), Row(id=u'P144674', Name=[u'Public administration- Other social services', u'General public administration sector'])]\n"}], "metadata": {"collapsed": false, "trusted": false}}, {"source": "#Step 6\n\n<h3>Creating simple graphs</h3>\nUsing Pandas we can do create some simple visualizations.", "cell_type": "markdown", "metadata": {}}, {"source": "####First create a SQL statement that is a resonable number if items\nFor example, you can count the number of projects (rows) by countryname\n<br>or in anothe words: \n<br>count(*), countryname from table group by countryname\n", "cell_type": "markdown", "metadata": {}}, {"execution_count": 21, "cell_type": "code", "source": "# we need to tell the charting library (matplotlib) to display charts inline\n# just run this paragraph\n%matplotlib inline \nimport matplotlib.pyplot as plt, numpy as np", "outputs": [], "metadata": {"collapsed": true, "trusted": false}}, {"execution_count": 22, "cell_type": "code", "source": "# first write the sql statment and look at the data, remember to add .toPandas() to have it look nice\n# an even easier option is to create a variable and set it to the SQL statement\n# for example: \n# query = \"select count(*) as Count, countryname from world_bank group by countryname\"\n# chart1_df = sqlContext.sql(query).toPandas()\n# print chart1_df", "outputs": [], "metadata": {"collapsed": false, "trusted": false}}, {"execution_count": 23, "cell_type": "code", "source": "# now take the variable (or same sql statement) and use the method:\n# .plot(kind='bar', x='countryname', y='Count', figsize=(12, 5))\n", "outputs": [], "metadata": {"collapsed": false, "trusted": false}}, {"source": "#Step 7\n\n<h3>Creating a dataframe \"manually\" by adding a schema to an RDD</h3>", "cell_type": "markdown", "metadata": {}}, {"source": "First, we need to create an RDD of pairs or triplets. This can be done using code (for loop) as\nseen in the instructor's example, or more simply by assigning values to an array.", "cell_type": "markdown", "metadata": {}}, {"execution_count": 2, "cell_type": "code", "source": "# Default array defined below. Feel free to change as desired.\narray=[[1,1,1],[2,2,2],[3,3,3],[4,4,4],[5,5,5]]\nmy_rdd = sc.parallelize(array)\nmy_rdd.collect()", "outputs": [{"execution_count": 2, "output_type": "execute_result", "data": {"text/plain": "[[1, 1, 1], [2, 2, 2], [3, 3, 3], [4, 4, 4], [5, 5, 5]]"}, "metadata": {}}], "metadata": {"collapsed": false, "trusted": false}}, {"source": "Use first the StructField method, following these steps:<br>\n1- Define your schema columns as a string<br>\n2- Build the schema object using StructField<br>\n3- Apply the schema object to the RDD<br>\n\nNote: The cell below is missing some code and will not run properly until the missing code has\nbeen completed.", "cell_type": "markdown", "metadata": {}}, {"execution_count": null, "cell_type": "code", "source": "from pyspark.sql.types import *\n\n# The schema is encoded in a string. Complete the string below\nschemaString = \"\"\n\n# MissingType() should be either StringType() or IntegerType(). Please replace as required.\nfields = [StructField(field_name, MissingType(), True) for field_name in schemaString.split()]\nschema = StructType(fields)\n\n# Apply the schema to the RDD.\nschemaExample = sqlContext.createDataFrame(use_your_rdd_name_here, schema)\n\n# Register the DataFrame as a table. Add table name below as parameter to registerTempTable.\nschemaExample.registerTempTable(\"\")\n", "outputs": [], "metadata": {"collapsed": true, "trusted": false}}, {"execution_count": null, "cell_type": "code", "source": "# Run some select statements on your newly created DataFrame and display the output", "outputs": [], "metadata": {"collapsed": true, "trusted": false}}, {"execution_count": null, "cell_type": "code", "source": "", "outputs": [], "metadata": {"collapsed": true, "trusted": false}}], "nbformat": 4, "metadata": {"kernelspec": {"display_name": "Python 2", "name": "python2", "language": "python"}, "language_info": {"mimetype": "text/x-python", "nbconvert_exporter": "python", "version": "2.7.11", "name": "python", "file_extension": ".py", "pygments_lexer": "ipython2", "codemirror_mode": {"version": 2, "name": "ipython"}}}}